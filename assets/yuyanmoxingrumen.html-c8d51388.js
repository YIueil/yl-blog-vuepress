import{_ as o,r as n,o as l,c,a as e,b as t,e as r,d as a}from"./app-4023f1b2.js";const i={},s=a('<h1 id="语言模型入门" tabindex="-1"><a class="header-anchor" href="#语言模型入门" aria-hidden="true">#</a> 语言模型入门</h1><blockquote><p>使用 ollama + deepseek + docker + Open WebUI 实现本地部署的对话模型。</p></blockquote><h2 id="_1-ollama安装配置" tabindex="-1"><a class="header-anchor" href="#_1-ollama安装配置" aria-hidden="true">#</a> 1 ollama安装配置</h2><p>下载适用于电脑软件平台的ollama安装包后，进行安装。</p><p>打开终端</p><ul><li>执行<code>Ollama -v</code>，将输出安装的Ollama版本号。</li><li>执行<code>Ollama list model</code>列举可运行的模型，此时应当是没有任何输出的，因为还没有下载和导入如题的语言模型文件。</li></ul><h3 id="两种deepseek模型下载方式" tabindex="-1"><a class="header-anchor" href="#两种deepseek模型下载方式" aria-hidden="true">#</a> 两种DeepSeek模型下载方式</h3><p><strong>1.1 直接通过Ollama进行下载（方式一）</strong> 选择适合自己跑的模型，数字越大需要的硬件性能越高：</p><table><thead><tr><th>模型参数</th><th>Windows配置要求</th><th>Mac配置要求</th><th>适用场景</th></tr></thead><tbody><tr><td>1.5B</td><td>- RAM: 4GB<br>- GPU: 集成显卡 (如GTX 1050) 或现代CPU<br>- 存储: 5GB</td><td>- 内存: 8GB (统一内存)<br>- 芯片: M1/M2/M3<br>- 存储: 5GB</td><td>简单文本生成/基础代码补全</td></tr><tr><td>7B</td><td>- RAM: 8-10GB<br>- GPU: GTX 1660 (4-bit量化)<br>- 存储: 8GB</td><td>- 内存: 16GB<br>- 芯片: M2 Pro/M3<br>- 存储: 8GB</td><td>中等复杂度问答/代码调试</td></tr><tr><td>8B</td><td>- RAM: 12GB<br>- GPU: RTX 3060 (8GB VRAM)<br>- 存储: 10GB</td><td>- 内存: 24GB<br>- 芯片: M2 Max<br>- 存储: 10GB</td><td>多轮对话/文档分析</td></tr><tr><td>14B</td><td>- RAM: 24GB<br>- GPU: RTX 3090 (24GB VRAM)<br>- 存储: 20GB</td><td>- 内存: 32GB<br>- 芯片: M3 Max<br>- 存储: 20GB</td><td>复杂推理/技术文档生成</td></tr><tr><td>32B</td><td>- RAM: 48GB<br>- GPU: RTX 4090 (4-bit量化)<br>- 存储: 40GB</td><td>- 内存: 64GB<br>- 芯片: M3 Ultra<br>- 存储: 40GB</td><td>科研计算/大规模数据处理</td></tr><tr><td>70B</td><td>- RAM: 64GB<br>- GPU: 双RTX 4090 (NVLINK)<br>- 存储: 80GB</td><td>- 内存: 128GB (需外接显存坞)<br>- 存储: 80GB</td><td>企业级AI服务/多模态处理</td></tr><tr><td>671B</td><td>- RAM: 256GB+<br>- GPU: 8xH100 (通过NVLINK连接)<br>- 存储: 1TB+</td><td>暂不支持</td><td>超大规模云端推理</td></tr></tbody></table><p><img src="https://s2.loli.net/2025/02/05/1mOxw3pSKTNzCcg.png" alt="1mOxw3pSKTNzCcg.png"></p><p>选好后终端执行命令<code>Ollama run deepseek-r1:1.5b</code>，我这里选择1.5b，做简单的翻译功能。</p>',11),h={href:"https://huggingface.co/",target:"_blank",rel:"noopener noreferrer"},p=a('<h3 id="运行模型" tabindex="-1"><a class="header-anchor" href="#运行模型" aria-hidden="true">#</a> 运行模型</h3><ol><li>终端执行命令<code>Ollama list</code>可以查看已经存在的模型。</li><li>终端执行命令<code>Ollama run &lt;模型名称&gt;</code>，运行对应的模型。如果模型不存在，则会自动进行对应模型的下载。</li></ol><p>此时已经可以在终端进行对话，但是不方便会话管理。 <img src="https://s2.loli.net/2025/02/06/pq6rNBTfhUkJ4dg.png" alt="pq6rNBTfhUkJ4dg.png"></p><h2 id="_2-open-webui-安装" tabindex="-1"><a class="header-anchor" href="#_2-open-webui-安装" aria-hidden="true">#</a> 2 Open WebUI 安装</h2>',4),b={href:"https://docs.openwebui.com/",target:"_blank",rel:"noopener noreferrer"},B=e("code",null,"docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main",-1),G=e("img",{src:"https://s2.loli.net/2025/02/05/kMrOBGCpa3uwo1T.png",alt:"kMrOBGCpa3uwo1T.png"},null,-1);function u(m,_){const d=n("ExternalLinkIcon");return l(),c("div",null,[s,e("p",null,[e("strong",null,[t("1.2 在"),e("a",h,[t("https://huggingface.co/"),r(d)]),t("进行下载（方式二）")]),t(" 待完善")]),p,e("p",null,[t("官网："),e("a",b,[t("🏡Open WebUI"),r(d)]),t(" 使用Open WebUI图形化页面来管理模型对话的记录。 使用Docker进行安装："),B,t(" 此时打开自己docker容器应用对应的端口，即可在浏览器可视化页面中进行对话的管理： "),G])])}const k=o(i,[["render",u],["__file","yuyanmoxingrumen.html.vue"]]);export{k as default};
